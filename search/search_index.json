{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/","title":"Aula 10   Introdu\u00e7\u00e3o ao Aprendizado de M\u00e1quina","text":"<p>O Aprendizado de M\u00e1quina (Machine Learning - ML) \u00e9 uma sub\u00e1rea da Intelig\u00eancia Artificial (IA) que permite que computadores aprendam a partir de dados, sem serem explicitamente programados. Esse conceito foi introduzido por Arthur Samuel (IBM) em 1959.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#1-tipos-de-aprendizado","title":"1. Tipos de Aprendizado","text":"<p>Existem tr\u00eas principais paradigmas de aprendizado:</p> <ol> <li>Aprendizado Supervisionado \u2192 Aprende a partir de dados rotulados.  </li> <li>Aprendizado N\u00e3o Supervisionado \u2192 Aprende a partir de dados n\u00e3o rotulados.  </li> <li>Aprendizado por Refor\u00e7o \u2192 Aprende pela intera\u00e7\u00e3o com o ambiente, recebendo recompensas.</li> </ol>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#2-aprendizado-supervisionado","title":"2. Aprendizado Supervisionado","text":"<p>No aprendizado supervisionado, o modelo \u00e9 treinado com dados rotulados, ou seja, para cada entrada h\u00e1 uma sa\u00edda esperada. Ele aprende a mapear entradas (X) para sa\u00eddas (Y).</p> <p>Objetivo: Estimar uma fun\u00e7\u00e3o que relacione entradas e sa\u00eddas, permitindo fazer previs\u00f5es em novos exemplos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#exemplo","title":"\ud83d\udd39 Exemplo:","text":"<p>Previs\u00e3o do pre\u00e7o de um apartamento com base em: - X1: Localiza\u00e7\u00e3o - X2: Tamanho em m\u00b2 - X3: N\u00famero de vagas</p> <p>O modelo aprende a prever o pre\u00e7o do im\u00f3vel (Y) com base nesses atributos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#principais-tarefas","title":"\ud83d\udccc Principais Tarefas:","text":"<p>\u2705 Regress\u00e3o \u2192 Quando a sa\u00edda Y \u00e9 cont\u00ednua (ex: prever pre\u00e7os). \u2705 Classifica\u00e7\u00e3o \u2192 Quando a sa\u00edda Y \u00e9 categ\u00f3rica (ex: prever se um e-mail \u00e9 spam ou n\u00e3o).</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#modelos-comuns","title":"\ud83d\udd39 Modelos comuns:","text":"<ul> <li>Regress\u00e3o Linear (para regress\u00e3o).</li> <li>\u00c1rvores de Decis\u00e3o (para classifica\u00e7\u00e3o e regress\u00e3o).</li> <li>Redes Neurais (usadas em diversas aplica\u00e7\u00f5es).</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#3-aprendizado-nao-supervisionado","title":"3. Aprendizado N\u00e3o Supervisionado","text":"<p>No aprendizado n\u00e3o supervisionado, os dados n\u00e3o possuem r\u00f3tulos. O objetivo \u00e9 descobrir padr\u00f5es nos dados.</p> <p>Objetivo: Identificar estruturas ocultas nos dados sem informa\u00e7\u00f5es expl\u00edcitas sobre a sa\u00edda.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#exemplo_1","title":"\ud83d\udd39 Exemplo:","text":"<p>Agrupar apartamentos com base em: - X1: Localiza\u00e7\u00e3o - X2: Tamanho em m\u00b2 - X3: N\u00famero de vagas</p> <p>O modelo pode agrupar im\u00f3veis similares sem saber seus pre\u00e7os.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#principais-tarefas_1","title":"\ud83d\udccc Principais Tarefas:","text":"<p>\u2705 Agrupamento (Clustering) \u2192 Segmenta\u00e7\u00e3o de clientes, organiza\u00e7\u00e3o de documentos. \u2705 Detec\u00e7\u00e3o de Anomalias \u2192 Identifica\u00e7\u00e3o de fraudes banc\u00e1rias. \u2705 Regras de Associa\u00e7\u00e3o \u2192 Descoberta de padr\u00f5es (ex: produtos comprados juntos no supermercado).</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#algoritmos-comuns","title":"\ud83d\udd39 Algoritmos comuns:","text":"<ul> <li>K-Means (para agrupamento).</li> <li>DBSCAN (para detec\u00e7\u00e3o de anomalias).</li> <li>Apriori (para regras de associa\u00e7\u00e3o).</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#4-comparacao-supervisionado-vs-nao-supervisionado","title":"4. Compara\u00e7\u00e3o: Supervisionado vs N\u00e3o Supervisionado","text":"==Supervisionado== ==N\u00e3o Supervisionado== Dados possuem r\u00f3tulos. Dados n\u00e3o possuem r\u00f3tulos. Aprende a prever uma sa\u00edda espec\u00edfica. Aprende a identificar padr\u00f5es. Exemplos: Regress\u00e3o, Classifica\u00e7\u00e3o. Exemplos: Agrupamento, Associa\u00e7\u00e3o."},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#5-aprendizado-por-reforco","title":"5. Aprendizado por Refor\u00e7o","text":"<p>No Aprendizado por Refor\u00e7o (Reinforcement Learning - RL), o agente aprende pela intera\u00e7\u00e3o com o ambiente e recebe recompensas conforme suas a\u00e7\u00f5es.</p> <p>Origem: Formalizado por Sutton &amp; Barto na d\u00e9cada de 1980.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#caracteristicas","title":"\ud83d\udd39 Caracter\u00edsticas:","text":"<ul> <li>Normalmente, o ambiente \u00e9 modelado como um Processo de Decis\u00e3o de Markov (MDP).</li> <li>O agente n\u00e3o conhece a fun\u00e7\u00e3o de transi\u00e7\u00e3o e as recompensas a priori.</li> <li>O aprendizado ocorre por tentativa e erro, ajustando as estrat\u00e9gias com base nas recompensas.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#exemplo_2","title":"\ud83d\udd39 Exemplo:","text":"<p>Um carro aut\u00f4nomo aprende a evitar colis\u00f5es ao dirigir em um ambiente simulado. Ele: - Recebe +1 ponto por completar o trajeto sem bater. - Recebe -1 ponto ao colidir.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#elementos-principais","title":"\ud83d\udccc Elementos principais:","text":"<p>\u2705 Estado (S) \u2192 Representa\u00e7\u00e3o do ambiente. \u2705 A\u00e7\u00e3o (A) \u2192 Escolha do agente em cada estado. \u2705 Recompensa (R) \u2192 Feedback positivo ou negativo.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#6-algoritmo-q-learning-aprendizado-por-reforco","title":"6. Algoritmo Q-Learning (Aprendizado por Refor\u00e7o)","text":"<p>O Q-Learning \u00e9 um dos principais algoritmos de Aprendizado por Refor\u00e7o.</p> <p>Objetivo: Estimar a fun\u00e7\u00e3o Q(s, a), que representa o valor de executar uma a\u00e7\u00e3o a em um estado s.</p> <p>A atualiza\u00e7\u00e3o dos valores Q(s, a) segue a equa\u00e7\u00e3o de Bellman:</p> <p>$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] $$</p> <p>Onde: - \u03b1 (taxa de aprendizado): Controle da atualiza\u00e7\u00e3o dos valores. - \u03b3 (fator de desconto): Quanto mais \u03b3 \u2192 mais valoriza recompensas futuras. - R \u2192 Recompensa imediata recebida ap\u00f3s tomar a a\u00e7\u00e3o.</p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#7-aplicacoes-do-aprendizado-de-maquina","title":"7. Aplica\u00e7\u00f5es do Aprendizado de M\u00e1quina","text":"<p>\u2705 Diagn\u00f3stico M\u00e9dico (classifica\u00e7\u00e3o de doen\u00e7as). \u2705 Detec\u00e7\u00e3o de Fraudes (an\u00e1lise de anomalias). \u2705 Previs\u00e3o Financeira (regress\u00e3o para prever pre\u00e7os de a\u00e7\u00f5es). \u2705 Rob\u00f3tica (aprendizado por refor\u00e7o para movimenta\u00e7\u00e3o aut\u00f4noma). \u2705 Processamento de Linguagem Natural (NLP) (tradu\u00e7\u00e3o autom\u00e1tica, chatbots).  </p>"},{"location":"Prova%202%20-%20IA/Aula%2010%20-%20Introdu%C3%A7%C3%A3o%20ao%20Aprendizado%20de%20M%C3%A1quina/#8-conclusao","title":"8. Conclus\u00e3o","text":"<p>O Aprendizado de M\u00e1quina \u00e9 um dos pilares da Intelig\u00eancia Artificial, permitindo a constru\u00e7\u00e3o de modelos preditivos e agentes aut\u00f4nomos.</p> <p>\u2705 Aprendizado Supervisionado \u2192 Usa dados rotulados para classifica\u00e7\u00e3o e regress\u00e3o. \u2705 Aprendizado N\u00e3o Supervisionado \u2192 Descobre padr\u00f5es ocultos sem r\u00f3tulos. \u2705 Aprendizado por Refor\u00e7o \u2192 Aprende com recompensas e penalidades.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/","title":"Aula 11a   Aprendizado Supervisionado   Classifica\u00e7\u00e3o","text":"<p>A classifica\u00e7\u00e3o \u00e9 uma tarefa do aprendizado supervisionado, onde o objetivo \u00e9 identificar a qual classe um determinado dado pertence, com base em dados rotulados.</p> <p>Exemplo: Dado um conjunto de imagens de gatos e cachorros, um modelo de classifica\u00e7\u00e3o pode aprender a identificar corretamente novas imagens.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#1-caracteristicas-do-problema-de-classificacao","title":"1. Caracter\u00edsticas do Problema de Classifica\u00e7\u00e3o","text":"<p>\u2705 Entrada: Um conjunto de exemplos de treinamento com r\u00f3tulos conhecidos. \u2705 Sa\u00edda: Um modelo que classifica novos exemplos em uma das categorias aprendidas. \u2705 Fases do Aprendizado:    - Treinamento: O modelo aprende a partir dos dados rotulados.    - Teste: O modelo \u00e9 avaliado em dados n\u00e3o vistos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#exemplo-pratico","title":"\ud83d\udd39 Exemplo Pr\u00e1tico","text":"Nome Bilheteria Avalia\u00e7\u00e3o G\u00eanero Toy Story 10.6 9 Anima\u00e7\u00e3o Se eu fosse voc\u00ea 30 8.7 Com\u00e9dia Cidade de Deus 12 8.2 Drama Juno 7 7 Com\u00e9dia Divertidamente 30 9.4 Anima\u00e7\u00e3o <p>O modelo pode aprender regras como: SE avalia\u00e7\u00e3o &gt; 8.9 ENT\u00c3O g\u00eanero = 'Anima\u00e7\u00e3o'.  </p> <p>Quando um novo filme surge (ex: \"Se Beber N\u00e3o Case\" com bilheteria 12 e avalia\u00e7\u00e3o 7.6), o modelo tentar\u00e1 classific\u00e1-lo corretamente.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#2-tipos-de-modelos-de-classificacao","title":"2. Tipos de Modelos de Classifica\u00e7\u00e3o","text":"<p>Os modelos de classifica\u00e7\u00e3o podem ser divididos em:</p> <p>\u2705 Modelos compreens\u00edveis (\"White Box\")    - \u00c1rvores de Decis\u00e3o \u2192 Criam regras de decis\u00e3o.    - Regras de Associa\u00e7\u00e3o \u2192 Descobrem padr\u00f5es expl\u00edcitos.    - Redes Bayesianas \u2192 Baseiam-se em probabilidades.</p> <p>\u2705 Modelos \"Caixa-Preta\" (\"Black Box\")    - SVM (Support Vector Machines) \u2192 Separa\u00e7\u00e3o entre classes por hiperplanos.    - Redes Neurais \u2192 Modelos altamente n\u00e3o lineares.    - KNN (K-Nearest Neighbors) \u2192 Classifica\u00e7\u00e3o baseada na proximidade dos vizinhos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#3-algoritmo-k-nearest-neighbors-knn","title":"3. Algoritmo K-Nearest Neighbors (KNN)","text":"<p>\ud83d\udccc Caracter\u00edsticas: - Algoritmo pregui\u00e7oso (Lazy Learning) \u2192 Apenas armazena os dados e classifica quando necess\u00e1rio. - Classifica um novo ponto com base na classe dos K vizinhos mais pr\u00f3ximos.</p> <p>\ud83d\udccc Funcionamento: 1. Calcula a dist\u00e2ncia entre o novo exemplo e os exemplos do conjunto de treinamento. 2. Encontra os K vizinhos mais pr\u00f3ximos. 3. A classe mais frequente entre os vizinhos define a classifica\u00e7\u00e3o.</p> <p>\ud83d\udccc Escolha do valor de K: - K muito pequeno \u2192 Pode levar a overfitting (ajuste excessivo aos dados). - K muito grande \u2192 Pode levar a underfitting (generaliza\u00e7\u00e3o ruim).  </p> <p>\ud83d\udca1 Dica: Em conjuntos desbalanceados, pode ser necess\u00e1rio usar vota\u00e7\u00e3o ponderada, onde vizinhos mais pr\u00f3ximos t\u00eam maior peso na decis\u00e3o.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#4-avaliacao-de-modelos-de-classificacao","title":"4. Avalia\u00e7\u00e3o de Modelos de Classifica\u00e7\u00e3o","text":"<p>A avalia\u00e7\u00e3o de classificadores \u00e9 essencial para medir sua efic\u00e1cia.</p> <p>\u2705 Matriz de Confus\u00e3o A matriz de confus\u00e3o mostra a rela\u00e7\u00e3o entre previs\u00f5es corretas e incorretas.</p> Classe Real Predi\u00e7\u00e3o Positiva Predi\u00e7\u00e3o Negativa Positiva (C1) VP (Verdadeiros Positivos) FN (Falsos Negativos) Negativa (C2) FP (Falsos Positivos) VN (Verdadeiros Negativos) <p>\ud83d\udccc M\u00e9tricas principais: - Acur\u00e1cia: Propor\u00e7\u00e3o de exemplos classificados corretamente.   $$   Acur\u00e1cia = \\frac{VP + VN}{VP + FP + VN + FN}   $$</p> <ul> <li> <p>Precis\u00e3o: % de previs\u00f5es positivas que realmente pertencem \u00e0 classe positiva.   $$   Precis\u00e3o = \\frac{VP}{VP + FP}   $$</p> </li> <li> <p>Revoca\u00e7\u00e3o (Recall): % dos exemplos positivos que foram corretamente classificados.   $$   Revoca\u00e7\u00e3o = \\frac{VP}{VP + FN}   $$</p> </li> <li> <p>F1-Score: M\u00e9dia harm\u00f4nica entre precis\u00e3o e revoca\u00e7\u00e3o.   $$   F1 = 2 \\times \\frac{\\text{Precis\u00e3o} \\times \\text{Revoca\u00e7\u00e3o}}{\\text{Precis\u00e3o} + \\text{Revoca\u00e7\u00e3o}}   $$</p> </li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#5-problema-das-classes-desbalanceadas","title":"5. Problema das Classes Desbalanceadas","text":"<p>\ud83d\udca1 Exemplo: - Suponha um classificador para c\u00e2ncer onde:   - 4 pacientes t\u00eam c\u00e2ncer.   - 500 pacientes n\u00e3o t\u00eam c\u00e2ncer.   - O modelo acerta 454 negativos, mas erra todos os positivos.</p> <p>Apesar da acur\u00e1cia de 90%, o modelo \u00e9 p\u00e9ssimo para identificar casos de c\u00e2ncer.</p> <p>\u2705 Solu\u00e7\u00e3o: Usar precis\u00e3o, recall e F1-score para avaliar melhor o desempenho.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011a%20-%20Aprendizado%20Supervisionado%20-%20Classifica%C3%A7%C3%A3o/#6-resumo-geral","title":"6. Resumo Geral","text":"<p>\ud83d\udccc Classifica\u00e7\u00e3o: - O problema de atribuir r\u00f3tulos a objetos com base em dados conhecidos. - Fases: Treinamento (modelo aprende) e Teste (modelo \u00e9 avaliado).</p> <p>\ud83d\udccc Modelos mais comuns: - \u00c1rvores de Decis\u00e3o, KNN, SVM, Redes Neurais.</p> <p>\ud83d\udccc Avalia\u00e7\u00e3o de Modelos: - Acur\u00e1cia \u2192 Classifica\u00e7\u00f5es corretas / total. - Precis\u00e3o \u2192 Quantos positivos previstos realmente s\u00e3o positivos. - Recall \u2192 Quantos exemplos positivos foram corretamente previstos. - F1-Score \u2192 Combina\u00e7\u00e3o de precis\u00e3o e recall.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/","title":"Aula 11b   Aprendizado N\u00e3o Supervisionado","text":"<p>O aprendizado n\u00e3o supervisionado busca encontrar padr\u00f5es e rela\u00e7\u00f5es em dados n\u00e3o rotulados. \u00c9 muito utilizado em minera\u00e7\u00e3o de dados, pois permite extrair informa\u00e7\u00f5es sem conhecimento pr\u00e9vio sobre os r\u00f3tulos.</p> <p>\ud83d\udccc Principais Tarefas: \u2705 Agrupamento (Clustering) \u2192 Separar os dados em grupos semelhantes. \u2705 Identifica\u00e7\u00e3o de Padr\u00f5es Frequentes \u2192 Encontrar padr\u00f5es comuns em grandes volumes de dados. \u2705 Identifica\u00e7\u00e3o de Anomalias \u2192 Detectar pontos fora do padr\u00e3o. \u2705 Redu\u00e7\u00e3o de Dimensionalidade \u2192 Simplificar os dados sem perder informa\u00e7\u00f5es essenciais.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#1-agrupamento-de-dados-clustering","title":"1. Agrupamento de Dados (Clustering)","text":"<p>O agrupamento de dados organiza um conjunto de exemplos em grupos naturais, de forma que os itens dentro do mesmo grupo sejam mais similares entre si do que com itens de outros grupos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#caracteristicas","title":"\ud83d\udd39 Caracter\u00edsticas","text":"<ul> <li>Problema n\u00e3o supervisionado \u2192 N\u00e3o h\u00e1 r\u00f3tulos indicando a qual grupo cada objeto pertence.</li> <li>Entrada \u2192 Um conjunto de dados sem r\u00f3tulos.</li> <li>Sa\u00edda \u2192 Uma atribui\u00e7\u00e3o de cada elemento a um grupo (cluster).</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#2-como-definir-os-grupos","title":"2. Como Definir os Grupos?","text":"<p>Para realizar o agrupamento, \u00e9 fundamental definir uma medida de similaridade entre os dados.</p> <p>\ud83d\udccc Exemplo de Medida de Similaridade: \u2705 Dist\u00e2ncia Euclidiana A dist\u00e2ncia entre dois pontos no espa\u00e7o \u00e9 medida como:</p> <p>$$ d(A, B) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$</p> <p>Essa m\u00e9trica \u00e9 usada por muitos algoritmos de agrupamento para calcular a proximidade entre exemplos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#3-algoritmo-k-means","title":"3. Algoritmo K-Means","text":"<p>O K-Means \u00e9 um dos algoritmos mais utilizados para agrupamento.</p> <p>\ud83d\udccc Funcionamento: 1. Escolhe aleatoriamente K pontos como os centroides iniciais. 2. Cada ponto do conjunto de dados \u00e9 associado ao centroide mais pr\u00f3ximo. 3. Os centroides s\u00e3o recalculados como a m\u00e9dia dos pontos atribu\u00eddos a cada grupo. 4. Repete at\u00e9 que os grupos n\u00e3o mudem mais.</p> <p>\ud83d\udccc Crit\u00e9rios para definir K: \u2705 Um K muito pequeno \u2192 Pode agrupar pontos diferentes no mesmo grupo. \u2705 Um K muito grande \u2192 Pode criar grupos com pouca distin\u00e7\u00e3o entre si.  </p> <p>\ud83d\udca1 M\u00e9todo das Silhuetas \u2192 Mede a qualidade dos clusters e ajuda a definir o melhor valor de K.</p> <ul> <li>Valor pr\u00f3ximo de 1 \u2192 O ponto est\u00e1 no grupo correto.</li> <li>Valor pr\u00f3ximo de 0 \u2192 O ponto est\u00e1 entre dois grupos.</li> <li>Valor negativo \u2192 O ponto foi colocado no grupo errado.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#4-avaliacao-dos-resultados-do-agrupamento","title":"4. Avalia\u00e7\u00e3o dos Resultados do Agrupamento","text":"<p>Como n\u00e3o h\u00e1 r\u00f3tulos nos dados, avaliar a qualidade do agrupamento \u00e9 desafiador. Algumas m\u00e9tricas ajudam a estimar a qualidade dos clusters:</p> <p>\ud83d\udccc Crit\u00e9rios de Qualidade: \u2705 Homogeneidade \u2192 Elementos dentro do mesmo grupo devem ser altamente similares. \u2705 Separa\u00e7\u00e3o \u2192 Elementos de diferentes grupos devem ser muito distintos.  </p> <p>\ud83d\udca1 Objetivo: Minimizar a dist\u00e2ncia entre pontos do mesmo grupo e maximizar a dist\u00e2ncia entre grupos distintos.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#5-identificacao-de-padroes-frequentes","title":"5. Identifica\u00e7\u00e3o de Padr\u00f5es Frequentes","text":"<p>\ud83d\udccc Objetivo: Encontrar padr\u00f5es comuns em grandes conjuntos de dados.</p> <p>\u2705 Exemplo: No supermercado, um sistema pode identificar que clientes que compram caf\u00e9 geralmente compram leite.  </p> <p>\ud83d\udccc T\u00e9cnicas usadas: \u2705 Regras de Associa\u00e7\u00e3o (como o Algoritmo Apriori) \u2705 M\u00e9tricas principais: - Suporte \u2192 Frequ\u00eancia com que um conjunto de itens aparece nos dados. - Confian\u00e7a \u2192 Probabilidade condicional de um item ser comprado junto a outro. - Lift \u2192 Mede a correla\u00e7\u00e3o entre os itens.</p> <p>\ud83d\udca1 Aplica\u00e7\u00e3o pr\u00e1tica \u2192 Sistemas de Recomenda\u00e7\u00e3o, como sugest\u00f5es de produtos na Amazon ou sugest\u00f5es de her\u00f3is em jogos online.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#6-identificacao-de-anomalias","title":"6. Identifica\u00e7\u00e3o de Anomalias","text":"<p>A identifica\u00e7\u00e3o de anomalias detecta itens ou eventos que diferem do padr\u00e3o em um conjunto de dados.</p> <p>\ud83d\udccc Aplica\u00e7\u00f5es: \u2705 Detec\u00e7\u00e3o de Fraudes \u2192 Transa\u00e7\u00f5es banc\u00e1rias suspeitas. \u2705 Seguran\u00e7a de Sistemas \u2192 Identifica\u00e7\u00e3o de acessos indevidos. \u2705 Monitoramento Industrial \u2192 Detec\u00e7\u00e3o de falhas em m\u00e1quinas.</p> <p>\ud83d\udca1 M\u00e9todo comum \u2192 Algoritmos de agrupamento para encontrar pontos fora do padr\u00e3o.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#7-reducao-de-dimensionalidade","title":"7. Redu\u00e7\u00e3o de Dimensionalidade","text":"<p>A redu\u00e7\u00e3o de dimensionalidade busca representar os dados em menos dimens\u00f5es, mantendo a informa\u00e7\u00e3o essencial.</p> <p>\ud83d\udccc Objetivo: \u2705 Facilitar a visualiza\u00e7\u00e3o dos dados. \u2705 Diminuir o custo computacional. \u2705 Evitar a maldi\u00e7\u00e3o da dimensionalidade (quando muitos atributos tornam o modelo ineficiente).</p> <p>\ud83d\udccc T\u00e9cnicas utilizadas: \u2705 PCA (Principal Component Analysis) \u2192 Converte os dados em novas vari\u00e1veis independentes. \u2705 Autoencoders \u2192 Redes neurais que aprendem uma representa\u00e7\u00e3o compacta dos dados.</p> <p>\ud83d\udca1 Exemplo Pr\u00e1tico: Para comparar diferentes algoritmos de tomada de decis\u00e3o em jogos, pesquisadores utilizaram PCA para reduzir um conjunto de 600.000 dados para 3 componentes principais, e depois aplicaram K-Means para agrupamento.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#resumo-geral-da-aula","title":"Resumo Geral da Aula","text":"<p>\ud83d\udccc Aprendizado N\u00e3o Supervisionado: - Objetivo: Encontrar padr\u00f5es em dados n\u00e3o rotulados. - Aplica\u00e7\u00f5es: Minera\u00e7\u00e3o de dados, recomenda\u00e7\u00e3o de produtos, detec\u00e7\u00e3o de fraudes.</p> <p>\ud83d\udccc Principais T\u00e9cnicas: \u2705 Agrupamento (Clustering) \u2192 K-Means, Dist\u00e2ncia Euclidiana. \u2705 Identifica\u00e7\u00e3o de Padr\u00f5es Frequentes \u2192 Regras de Associa\u00e7\u00e3o. \u2705 Identifica\u00e7\u00e3o de Anomalias \u2192 Detec\u00e7\u00e3o de fraudes e eventos incomuns. \u2705 Redu\u00e7\u00e3o de Dimensionalidade \u2192 PCA, Autoencoders.</p> <p>\ud83d\ude80 Impacto na Intelig\u00eancia Artificial: Essas t\u00e9cnicas s\u00e3o essenciais para an\u00e1lise de dados em grande escala, sendo usadas em sistemas de recomenda\u00e7\u00e3o, seguran\u00e7a e aprendizado profundo.</p>"},{"location":"Prova%202%20-%20IA/Aula%2011b%20-%20Aprendizado%20N%C3%A3o%20Supervisionado/#conclusao","title":"Conclus\u00e3o","text":"<p>O aprendizado n\u00e3o supervisionado \u00e9 uma \u00e1rea poderosa da IA que permite a descoberta de padr\u00f5es e insights sem necessidade de r\u00f3tulos. Seus m\u00e9todos s\u00e3o fundamentais para aplica\u00e7\u00f5es como reconhecimento de padr\u00f5es, recomenda\u00e7\u00f5es autom\u00e1ticas e seguran\u00e7a cibern\u00e9tica.</p> <p>\ud83d\udd25 Principais aprendizados: \u2705 O K-Means \u00e9 amplamente utilizado para agrupamento de dados. \u2705 A escolha correta de K \u00e9 fundamental para bons agrupamentos. \u2705 Regras de Associa\u00e7\u00e3o ajudam na descoberta de padr\u00f5es frequentes. \u2705 Detec\u00e7\u00e3o de Anomalias \u00e9 usada em seguran\u00e7a e monitoramento. \u2705 Redu\u00e7\u00e3o de Dimensionalidade melhora a efici\u00eancia da an\u00e1lise de dados.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/","title":"Aula 12   Aprendizado por Refor\u00e7o","text":"<p>Aprendizado por Refor\u00e7o (Reinforcement Learning - RL) \u00e9 um paradigma onde agentes aprendem ao interagir com o ambiente, tomando a\u00e7\u00f5es e recebendo recompensas de acordo com os resultados.  </p> <p>\ud83d\udccc Objetivo: Aprender a pol\u00edtica \u00f3tima de a\u00e7\u00f5es que maximiza a recompensa total ao longo do tempo.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#1-definicao-do-problema","title":"1. Defini\u00e7\u00e3o do Problema","text":"<p>O Aprendizado por Refor\u00e7o pode ser modelado como um Processo de Decis\u00e3o de Markov (MDP).</p> <p>\ud83d\udccc Elementos do RL: \u2705 Estados (S) \u2192 Representa\u00e7\u00e3o do ambiente em um dado momento. \u2705 A\u00e7\u00f5es (A) \u2192 Conjunto de escolhas poss\u00edveis para o agente. \u2705 Recompensas (R) \u2192 Feedback do ambiente ap\u00f3s uma a\u00e7\u00e3o. \u2705 Pol\u00edtica (\u03c0) \u2192 Regra que define a a\u00e7\u00e3o tomada para cada estado. \u2705 Modelo de Transi\u00e7\u00e3o (T) \u2192 Probabilidade de transi\u00e7\u00e3o entre estados (pode ser desconhecido).  </p> <p>\ud83d\udca1 Diferen\u00e7a do Aprendizado Supervisionado: - No supervisionado, h\u00e1 um conjunto de dados rotulados. - No refor\u00e7o, o agente descobre a resposta ao interagir com o ambiente.  </p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#2-quando-usar-o-aprendizado-por-reforco","title":"2. Quando Usar o Aprendizado por Refor\u00e7o?","text":"<p>\ud83d\udccc O RL \u00e9 indicado quando: \u2705 Os dados possuem depend\u00eancia temporal (sequ\u00eancias ou trajet\u00f3rias). \u2705 As decis\u00f5es do agente influenciam estados futuros. \u2705 O modelo do ambiente n\u00e3o \u00e9 totalmente conhecido.  </p> <p>\ud83d\udca1 Exemplo: Treinar um rob\u00f4 para andar \u2192 O rob\u00f4 aprende quais movimentos s\u00e3o mais eficientes para evitar quedas e avan\u00e7ar.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#3-principais-abordagens","title":"3. Principais Abordagens","text":"<p>O RL pode ser resolvido por diferentes abordagens:</p> <p>\u2705 Programa\u00e7\u00e3o Din\u00e2mica \u2192 Se o modelo do ambiente for conhecido, pode-se calcular a pol\u00edtica \u00f3tima com:    - Value Iteration    - Policy Iteration</p> <p>\u2705 M\u00e9todos Baseados em Amostragem (quando o modelo n\u00e3o \u00e9 conhecido):    - Monte Carlo \u2192 Aprendizado a partir de epis\u00f3dios completos.    - Temporal Difference (TD Learning) \u2192 Atualiza estimativas de recompensa ao longo do tempo.      - Q-Learning (off-policy)      - SARSA (on-policy)</p> <p>\u2705 Aproxima\u00e7\u00e3o de Fun\u00e7\u00f5es:    - Modelos Lineares    - Deep Reinforcement Learning (Deep RL) (usando redes neurais).</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#4-aprendizado-temporal-difference-td-learning","title":"4. Aprendizado Temporal Difference (TD Learning)","text":"<p>\ud83d\udccc TD Learning combina elementos de: \u2705 Monte Carlo \u2192 Atualiza valores com base na experi\u00eancia. \u2705 Programa\u00e7\u00e3o Din\u00e2mica \u2192 Utiliza estimativas para aprender valores de estado.</p> <p>A atualiza\u00e7\u00e3o \u00e9 feita pela diferen\u00e7a temporal entre a previs\u00e3o e a recompensa observada.</p> <p>\ud83d\udccc F\u00f3rmula do TD Update:</p> <p>$$ V(s) \\leftarrow V(s) + \\alpha \\left[ R + \\gamma V(s') - V(s) \\right] $$</p> <p>Onde: - $V(s)$ \u2192 Valor estimado do estado atual. - $\\alpha$ \u2192 Taxa de aprendizado. - $R$ \u2192 Recompensa imediata. - $\\gamma$ \u2192 Fator de desconto (valoriza\u00e7\u00e3o de recompensas futuras). - $V(s')$ \u2192 Valor estimado do pr\u00f3ximo estado.</p> <p>\ud83d\udca1 O erro de TD mede a diferen\u00e7a entre o valor atual e a expectativa futura.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#5-funcao-de-valor-e-q-function","title":"5. Fun\u00e7\u00e3o de Valor e Q-Function","text":"<p>\ud83d\udccc Fun\u00e7\u00e3o de Valor de Estado $V(s)$: Mede o valor esperado das recompensas ao seguir uma pol\u00edtica \u03c0 a partir do estado s.</p> <p>$$ V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid s_0 = s \\right] $$</p> <p>\ud83d\udccc Fun\u00e7\u00e3o de Valor de A\u00e7\u00f5es $Q(s,a)$: Mede o valor esperado ao escolher uma a\u00e7\u00e3o a em um estado s, seguindo uma pol\u00edtica \u03c0.</p> <p>$$ Q^\\pi(s,a) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid s_0 = s, a_0 = a \\right] $$</p> <p>\ud83d\udca1 Diferen\u00e7a: \u2705 $V(s)$ \u2192 Mede o valor do estado. \u2705 $Q(s,a)$ \u2192 Mede o valor de uma a\u00e7\u00e3o em um estado.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#6-q-learning","title":"6. Q-Learning","text":"<p>O Q-Learning \u00e9 um dos algoritmos mais usados em RL. Ele permite aprender Q(s, a) sem precisar conhecer o modelo de transi\u00e7\u00e3o.</p> <p>\ud83d\udccc F\u00f3rmula de Atualiza\u00e7\u00e3o do Q-Learning:</p> <p>$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] $$</p> <p>\ud83d\udca1 Caracter\u00edsticas: \u2705 Off-policy \u2192 Aprende com a melhor a\u00e7\u00e3o futura poss\u00edvel e n\u00e3o apenas com as a\u00e7\u00f5es tomadas pelo agente. \u2705 Model-free \u2192 N\u00e3o precisa conhecer a fun\u00e7\u00e3o de transi\u00e7\u00e3o do ambiente.  </p> <p>\ud83d\udccc Passos do Algoritmo Q-Learning: 1. Inicializa $Q(s,a)$ para todos os estados e a\u00e7\u00f5es. 2. No estado s, escolhe uma a\u00e7\u00e3o a. 3. Executa a, recebe recompensa R e observa novo estado s'. 4. Atualiza $Q(s,a)$ usando a equa\u00e7\u00e3o acima. 5. Repete at\u00e9 converg\u00eancia.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#7-exploracao-vs-exploracao-exploration-vs-exploitation","title":"7. Explora\u00e7\u00e3o vs Explora\u00e7\u00e3o (Exploration vs Exploitation)","text":"<p>Para aprender bem, o agente precisa equilibrar: \u2705 Explora\u00e7\u00e3o (Exploration) \u2192 Testar novas a\u00e7\u00f5es para descobrir melhores estrat\u00e9gias. \u2705 Explora\u00e7\u00e3o (Exploitation) \u2192 Escolher as melhores a\u00e7\u00f5es j\u00e1 conhecidas.</p> <p>\ud83d\udccc T\u00e9cnica $\\epsilon$-Greedy: 6. Com probabilidade $\\epsilon$, escolhe uma a\u00e7\u00e3o aleat\u00f3ria (explora\u00e7\u00e3o). 7. Com probabilidade $1 - \\epsilon$, escolhe a melhor a\u00e7\u00e3o conhecida (explora\u00e7\u00e3o).</p> <p>\ud83d\udca1 Inicialmente $\\epsilon$ deve ser alto para mais explora\u00e7\u00e3o, e depois diminuir para mais explora\u00e7\u00e3o.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#8-exemplos-de-aplicacao","title":"8. Exemplos de Aplica\u00e7\u00e3o","text":"<p>\u2705 Grid World \u2192 O agente aprende a navegar em um ambiente para alcan\u00e7ar a maior recompensa. \u2705 Pac-Man \u2192 O agente aprende estrat\u00e9gias para evitar fantasmas e maximizar a pontua\u00e7\u00e3o. \u2705 Rob\u00f3tica \u2192 Treinamento de rob\u00f4s para locomo\u00e7\u00e3o eficiente. \u2705 Financeiro \u2192 Algoritmos que aprendem estrat\u00e9gias de negocia\u00e7\u00e3o. \u2705 Jogos \u2192 Aprendizado de IA em Xadrez, Go, Starcraft e Dota 2.</p>"},{"location":"Prova%202%20-%20IA/Aula%2012%20-%20Aprendizado%20por%20Refor%C3%A7o/#resumo-geral","title":"Resumo Geral","text":"<p>\ud83d\udccc Aprendizado por Refor\u00e7o (RL): - Baseado em intera\u00e7\u00e3o com o ambiente e recebimento de recompensas. - Pode ser modelado como um MDP. - Objetivo: Encontrar a melhor pol\u00edtica $\\pi^*$ para maximizar as recompensas.</p> <p>\ud83d\udccc Principais T\u00e9cnicas: \u2705 Temporal Difference Learning \u2192 Atualiza valores de estado com base na diferen\u00e7a entre previs\u00f5es e recompensas reais. \u2705 Q-Learning \u2192 Algoritmo off-policy, baseado na atualiza\u00e7\u00e3o da fun\u00e7\u00e3o de valor de a\u00e7\u00e3o. \u2705 Explora\u00e7\u00e3o vs Explora\u00e7\u00e3o \u2192 Estrat\u00e9gias como $\\epsilon$-Greedy balanceiam descoberta e aproveitamento.</p> <p>\ud83d\ude80 Aplica\u00e7\u00f5es pr\u00e1ticas incluem: Jogos, rob\u00f3tica, finan\u00e7as, assistentes inteligentes e mu</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/","title":"Aula 7 e 8   Racioc\u00ednio Probabil\u00edstico","text":"<p>O estudo do racioc\u00ednio probabil\u00edstico na Intelig\u00eancia Artificial (IA) trata da forma como agentes lidam com a incerteza em suas percep\u00e7\u00f5es e decis\u00f5es. Essa incerteza ocorre devido a informa\u00e7\u00f5es parciais, din\u00e2micas ou imperfeitas do ambiente. O uso da teoria das probabilidades permite modelar essa incerteza e construir sistemas inteligentes mais eficientes.</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#1-quantificando-a-incerteza","title":"1. Quantificando a Incerteza","text":"<p>Agentes inteligentes operam sob graus de cren\u00e7a, atualizados conforme novas evid\u00eancias.</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#teoria-das-probabilidades","title":"\ud83d\udd39 Teoria das Probabilidades","text":"<ul> <li>Atribui um valor num\u00e9rico (0 a 1) ao grau de cren\u00e7a do agente sobre determinado evento.</li> <li>Probabilidade a priori (Prior): estimativa inicial antes de qualquer observa\u00e7\u00e3o.</li> <li>Probabilidade a posteriori (Posterior): cren\u00e7a revisada ap\u00f3s novas evid\u00eancias.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#exemplo","title":"\ud83d\udd39 Exemplo","text":"<ul> <li>Antes de observar nuvens: P(Chuva) = 0.3</li> <li>Ap\u00f3s ver nuvens escuras: P(Chuva | Nuvens) = 0.7</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#2-decisao-racional","title":"2. Decis\u00e3o Racional","text":"<p>Um agente racional deve tomar decis\u00f5es baseadas em: 1. Fun\u00e7\u00e3o de utilidade \u2013 representa as prefer\u00eancias do agente. 2. Probabilidades \u2013 mede a chance de um evento ocorrer.</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#utilidade-esperada","title":"\ud83d\udd39 Utilidade Esperada","text":"<p>$$ EU(a) = \\sum P(resultado | a\u00e7\u00e3o) \\times U(resultado) $$</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#exemplo_1","title":"\ud83d\udd39 Exemplo","text":"<ul> <li>P(Chuva | Nuvens) = 0.7</li> <li>Levar um guarda-chuva tem um pequeno custo (-1), mas n\u00e3o levar pode causar grande preju\u00edzo (-10).</li> <li>A melhor decis\u00e3o \u00e9 levar o guarda-chuva.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#3-revisao-de-conceitos-de-probabilidades","title":"3. Revis\u00e3o de Conceitos de Probabilidades","text":"<ul> <li>Espa\u00e7o amostral (\u03a9): conjunto de todos os resultados poss\u00edveis.</li> <li>Exemplo:</li> <li>Moeda: \u03a9 = {cara, coroa}, P(cara) = 0.5.</li> <li>Dois dados: \u03a9 = {11, 12, ..., 66}, P(dupla de n\u00fameros iguais) = 6/36.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#variaveis-aleatorias","title":"\ud83d\udd39 Vari\u00e1veis Aleat\u00f3rias","text":"<ul> <li>Booleanas: Gripe = {true, false}.</li> <li>Discretas: Tempo = {ensolarado, nublado, chuvoso}.</li> <li>Cont\u00ednuas: Sensor = [0,1].</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#4-probabilidade-condicional-e-regra-de-bayes","title":"4. Probabilidade Condicional e Regra de Bayes","text":"<p>A probabilidade condicional mede a chance de um evento ocorrer dado que outro evento j\u00e1 aconteceu:</p> <p>$$ P(A | B) = \\frac{P(A \\cap B)}{P(B)} $$</p> <p>A Regra de Bayes permite inferir a causa mais prov\u00e1vel:</p> <p>$$ P(H | E) = \\frac{P(E | H) P(H)}{P(E)} $$</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#exemplo-diagnostico-medico","title":"\ud83d\udd39 Exemplo: Diagn\u00f3stico M\u00e9dico","text":"<ul> <li>P(Meningite) = 1/50.000</li> <li>P(Pesco\u00e7o duro | Meningite) = 0.5</li> <li>P(Pesco\u00e7o duro) = 1/20</li> <li>Aplicando Bayes, P(Meningite | Pesco\u00e7o duro) \u2248 0.002 (0.2%).</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#5-independencia-e-independencia-condicional","title":"5. Independ\u00eancia e Independ\u00eancia Condicional","text":"<ul> <li>Eventos independentes: quando um evento n\u00e3o afeta a probabilidade do outro.</li> </ul> <p>$$ P(A \\cap B) = P(A) P(B) $$</p> <ul> <li>Independ\u00eancia condicional: dois eventos podem ser independentes se uma terceira vari\u00e1vel for conhecida.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#exemplo_2","title":"\ud83d\udd39 Exemplo","text":"<ul> <li>Febre e tosse s\u00e3o condicionalmente independentes dado que a pessoa tem gripe.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#6-processos-de-inferencia-temporal","title":"6. Processos de Infer\u00eancia Temporal","text":"<p>Quando um ambiente muda ao longo do tempo, o agente deve atualizar suas cren\u00e7as constantemente.</p> <ul> <li>Vari\u00e1veis de estado (X\u209c) representam o ambiente.</li> <li>Vari\u00e1veis de evid\u00eancia (E\u209c) representam observa\u00e7\u00f5es.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#exemplo_3","title":"\ud83d\udd39 Exemplo","text":"<ul> <li>Um guarda subterr\u00e2neo observa se o diretor carrega um guarda-chuva para inferir se est\u00e1 chovendo.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#7-cadeias-de-markov","title":"7. Cadeias de Markov","text":"<ul> <li>Princ\u00edpio de Markov: o estado atual depende apenas do estado anterior.</li> <li>Cadeias de Markov s\u00e3o utilizadas para modelar transi\u00e7\u00f5es entre estados ao longo do tempo.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#8-tecnicas-de-inferencia-em-modelos-temporais","title":"8. T\u00e9cnicas de Infer\u00eancia em Modelos Temporais","text":"<ol> <li>Filtragem: estimar o estado atual com base em todas as observa\u00e7\u00f5es passadas.</li> <li>Predi\u00e7\u00e3o: prever estados futuros com base em evid\u00eancias passadas.</li> <li>Suaviza\u00e7\u00e3o: inferir estados passados considerando evid\u00eancias futuras.</li> <li>Explica\u00e7\u00e3o mais prov\u00e1vel: encontrar a sequ\u00eancia de estados mais prov\u00e1vel.</li> </ol>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#9-modelos-especificos-para-inferencia-temporal","title":"9. Modelos Espec\u00edficos para Infer\u00eancia Temporal","text":"<ul> <li>Modelos Ocultos de Markov (HMM): usados quando h\u00e1 estados ocultos.</li> <li>Filtros de Kalman: aplic\u00e1veis a vari\u00e1veis cont\u00ednuas.</li> <li>Filtros de Part\u00edculas: m\u00e9todos de estimativa por amostragem.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#10-filtragem-bayesiana","title":"10. Filtragem Bayesiana","text":"<p>A Filtragem Bayesiana permite atualizar cren\u00e7as ao longo do tempo:</p> <p>$$ P(X_t | e_{1:t}) = \\alpha P(e_t | X_t) \\sum P(X_t | X_{t-1}) P(X_{t-1} | e_{1:t-1}) $$</p> <p>Onde: - P(X_t | e_{1:t}): cren\u00e7a sobre o estado atual. - P(e_t | X_t): modelo de observa\u00e7\u00e3o. - P(X_t | X_{t-1}): modelo de transi\u00e7\u00e3o.</p>"},{"location":"Prova%202%20-%20IA/Aula%207%20e%208%20-%20Racioc%C3%ADnio%20Probabil%C3%ADstico/#conclusao","title":"Conclus\u00e3o","text":"<p>Os m\u00e9todos probabil\u00edsticos s\u00e3o fundamentais para a Intelig\u00eancia Artificial, pois permitem: \u2705 Modelar a incerteza. \u2705 Tomar decis\u00f5es informadas com base em evid\u00eancias. \u2705 Fazer previs\u00f5es e infer\u00eancias em ambientes din\u00e2micos.  </p> <p>Os principais modelos estudados incluem: - Redes Bayesianas (para infer\u00eancia probabil\u00edstica). - Cadeias de Markov (para processos din\u00e2micos). - Filtros de Kalman e Part\u00edculas (para infer\u00eancias em tempo real).</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/","title":"Aula 9   Processos de Decis\u00e3o de Markov (MDPs)","text":"<p>Os Processos de Decis\u00e3o de Markov (MDPs) s\u00e3o usados para modelar agentes que tomam decis\u00f5es sequenciais em ambientes incertos, onde cada a\u00e7\u00e3o influencia estados futuros. Eles s\u00e3o amplamente utilizados em Aprendizado por Refor\u00e7o (Reinforcement Learning).</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#1-tomada-de-decisao","title":"1. Tomada de Decis\u00e3o","text":""},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#problema","title":"\ud83d\udd39 Problema","text":"<ul> <li>Um agente est\u00e1 em um estado e pode escolher entre v\u00e1rias a\u00e7\u00f5es.</li> <li>Seu objetivo \u00e9 escolher a a\u00e7\u00e3o que maximiza a recompensa acumulada.</li> </ul> <p>Existem dois tipos principais de tomada de decis\u00e3o: 1. Decis\u00e3o \u00fanica (\"single-shot\"): Problema dos Multi-Armed Bandits. 2. Decis\u00e3o sequencial: Modelada por Processos de Decis\u00e3o de Markov (MDPs).</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#2-multi-armed-bandits","title":"2. Multi-Armed Bandits","text":"<p>Um Multi-Armed Bandit \u00e9 uma met\u00e1fora para um agente que precisa decidir entre v\u00e1rias op\u00e7\u00f5es desconhecidas.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#caracteristicas","title":"\ud83d\udd39 Caracter\u00edsticas","text":"<ul> <li>Cada alavanca de uma m\u00e1quina ca\u00e7a-n\u00edquel d\u00e1 um recompensa aleat\u00f3ria com m\u00e9dia e vari\u00e2ncia diferentes.</li> <li>O agente deve decidir quando explorar novas op\u00e7\u00f5es e quando explorar a melhor op\u00e7\u00e3o conhecida (dilema explora\u00e7\u00e3o x explora\u00e7\u00e3o).</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#estrategias","title":"\ud83d\udd39 Estrat\u00e9gias","text":"<ol> <li>\u03b5-greedy:  </li> <li>Com probabilidade \u03b5, escolhe uma a\u00e7\u00e3o aleat\u00f3ria (explora\u00e7\u00e3o).  </li> <li> <p>Com probabilidade 1 - \u03b5, escolhe a a\u00e7\u00e3o de maior recompensa (explora\u00e7\u00e3o).</p> </li> <li> <p>Upper Confidence Bound (UCB):  </p> </li> <li>Escolhe a a\u00e7\u00e3o com maior m\u00e9dia + vari\u00e2ncia, equilibrando explora\u00e7\u00e3o e explora\u00e7\u00e3o.</li> </ol>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#3-processos-de-decisao-de-markov-mdps","title":"3. Processos de Decis\u00e3o de Markov (MDPs)","text":"<p>Os MDPs s\u00e3o modelos matem\u00e1ticos usados para representar decis\u00f5es sequenciais sob incerteza.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#propriedade-de-markov","title":"\ud83d\udd39 Propriedade de Markov","text":"<p>A transi\u00e7\u00e3o de estado depende apenas do estado atual e da a\u00e7\u00e3o tomada, n\u00e3o dos estados passados.</p> <p>$$ P(s' | s, a) = T(s, a, s') $$</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#definicao-formal","title":"\ud83d\udd39 Defini\u00e7\u00e3o Formal","text":"<p>Um MDP \u00e9 definido como um tupla \u27e8S, A, T, R\u27e9: - S: Conjunto de estados. - A: Conjunto de a\u00e7\u00f5es. - T: Fun\u00e7\u00e3o de transi\u00e7\u00e3o \u2192 Probabilidade de chegar ao estado s' ao tomar a a\u00e7\u00e3o a no estado s:</p> <p>$$   T(s,a,s') = P(s' | s,a)   $$</p> <ul> <li>R: Fun\u00e7\u00e3o de recompensa \u2192 Recompensa recebida ao estar no estado s:</li> </ul> <p>$$   R(s)   $$</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#exemplo-mundo-4x3","title":"\ud83d\udd39 Exemplo: Mundo 4x3","text":"<ul> <li>Estados (S): { (1,1), (1,2), ..., (4,3) }.</li> <li>A\u00e7\u00f5es (A): { Cima, Baixo, Esquerda, Direita }.</li> <li>Recompensas (R):</li> <li>+1 no estado (4,3).</li> <li>-1 no estado (4,2).</li> <li>-0.04 nos demais estados.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#4-recompensas-acumuladas-e-descontadas","title":"4. Recompensas Acumuladas e Descontadas","text":"<p>Ao longo do tempo, um agente acumula recompensas \u00e0 medida que interage com o ambiente.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#recompensa-cumulativa","title":"\ud83d\udd39 Recompensa Cumulativa","text":"<p>Para um horizonte finito (T passos), a recompensa acumulada \u00e9:</p> <p>$$ G_t = R_{t+1} + R_{t+2} + ... + R_T $$</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#recompensa-descontada","title":"\ud83d\udd39 Recompensa Descontada","text":"<p>Para um horizonte infinito, recompensas futuras s\u00e3o penalizadas por um fator de desconto \u03b3:</p> <p>$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... $$</p> <ul> <li>\u03b3 (fator de desconto):  </li> <li>\u03b3 \u2192 0 \u2192 O agente s\u00f3 valoriza recompensas imediatas.  </li> <li>\u03b3 \u2192 1 \u2192 O agente valoriza tamb\u00e9m recompensas futuras.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#5-politicas-de-decisao","title":"5. Pol\u00edticas de Decis\u00e3o","text":"<p>Uma pol\u00edtica \u03c0 define o comportamento do agente.</p> <ul> <li>Defini\u00e7\u00e3o: Mapeia estados para a\u00e7\u00f5es.   $$   a = \\pi(s)   $$</li> <li>Objetivo: Encontrar a pol\u00edtica \u00f3tima \u03c0* que maximiza a recompensa ao longo do tempo.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#exemplo-de-politica","title":"\ud83d\udd39 Exemplo de Pol\u00edtica","text":"<ul> <li>Se o agente segue a pol\u00edtica:</li> <li>\u03c0(s) = direita no estado (1,1).</li> <li>\u03c0(s) = cima no estado (2,1).</li> <li>...</li> </ul> <p>Isso define uma estrat\u00e9gia para navegar no ambiente.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#6-funcao-de-valor","title":"6. Fun\u00e7\u00e3o de Valor","text":"<p>A fun\u00e7\u00e3o de valor mede a qualidade de um estado a longo prazo.</p> <ul> <li>Defini\u00e7\u00e3o: O valor de um estado s \u00e9 a soma esperada das recompensas descontadas ao seguir a pol\u00edtica \u03c0:</li> </ul> <p>$$   V^\\pi(s) = \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) \\mid s_0 = s \\right]   $$</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#equacao-de-bellman","title":"\ud83d\udd39 Equa\u00e7\u00e3o de Bellman","text":"<p>A fun\u00e7\u00e3o de valor pode ser definida recursivamente:</p> <p>$$ V^\\pi(s) = R(s) + \\gamma \\sum_{s'} P(s' | s, \\pi(s)) V^\\pi(s') $$</p> <p>Isso permite calcular valores de estados de maneira iterativa.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#7-politicas-e-funcoes-de-valor-otimas","title":"7. Pol\u00edticas e Fun\u00e7\u00f5es de Valor \u00d3timas","text":"<p>A pol\u00edtica \u00f3tima \u00e9 aquela que maximiza a fun\u00e7\u00e3o de valor:</p> <p>$$ V^(s) = \\max_a \\sum_{s'} P(s' | s, a) [ R(s) + \\gamma V^(s') ] $$</p> <ul> <li>A pol\u00edtica \u00f3tima \u03c0 \u00e9 obtida escolhendo sempre a melhor a\u00e7\u00e3o*.</li> <li>Essa equa\u00e7\u00e3o \u00e9 conhecida como a Equa\u00e7\u00e3o da Otimalidade de Bellman.</li> </ul>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#8-programacao-dinamica-para-resolver-mdps","title":"8. Programa\u00e7\u00e3o Din\u00e2mica para Resolver MDPs","text":"<p>Se conhecemos a fun\u00e7\u00e3o de transi\u00e7\u00e3o T(s, a, s') e a fun\u00e7\u00e3o de recompensa R(s), podemos encontrar a pol\u00edtica \u00f3tima com Programa\u00e7\u00e3o Din\u00e2mica.</p>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#metodos-de-programacao-dinamica","title":"\ud83d\udd39 M\u00e9todos de Programa\u00e7\u00e3o Din\u00e2mica","text":"<ol> <li>Itera\u00e7\u00e3o de Valor (Value Iteration) </li> <li>Inicia com valores aleat\u00f3rios e atualiza iterativamente usando a Equa\u00e7\u00e3o de Bellman.</li> <li> <p>Converge para V e \u03c0**.</p> </li> <li> <p>Itera\u00e7\u00e3o de Pol\u00edtica (Policy Iteration) </p> </li> <li>Alterna entre avalia\u00e7\u00e3o da pol\u00edtica e melhoria da pol\u00edtica.</li> <li>Garante converg\u00eancia para a pol\u00edtica \u00f3tima.</li> </ol>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#9-iteracao-de-valor","title":"9. Itera\u00e7\u00e3o de Valor","text":"<p>O algoritmo de Value Iteration funciona da seguinte forma:</p> <ol> <li>Inicializa V(s) aleatoriamente para todos os estados.</li> <li>Para cada estado s, aplica a Equa\u00e7\u00e3o da Otimalidade de Bellman:</li> </ol> <p>$$    V(s) \\leftarrow \\max_a \\sum_{s'} P(s' | s, a) [ R(s) + \\gamma V(s') ]    $$</p> <ol> <li>Repete at\u00e9 que as mudan\u00e7as sejam pequenas (abaixo de um limite).</li> </ol>"},{"location":"Prova%202%20-%20IA/Aula%209%20-%20Processos%20de%20Decis%C3%A3o%20de%20Markov%20%28MDPs%29/#conclusao","title":"Conclus\u00e3o","text":"<p>Os Processos de Decis\u00e3o de Markov (MDPs) s\u00e3o fundamentais para a modelagem de decis\u00f5es sequenciais. Eles fornecem um arcabou\u00e7o matem\u00e1tico para agentes que aprendem a agir em ambientes incertos.</p> <p>\u2705 Modelam problemas de decis\u00e3o sob incerteza \u2705 Usam recompensas descontadas para avaliar estrat\u00e9gias \u2705 Podem ser resolvidos via Programa\u00e7\u00e3o Din\u00e2mica </p>"}]}