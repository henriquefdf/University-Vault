
<!doctype html>
<html lang="pt" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Aula 12 Aprendizado por Refor√ßo - Resumos UFMG</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-definicao-do-problema" class="md-skip">
          Ir para o conte√∫do
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabe√ßalho">
    <a href="../.." title="Resumos UFMG" class="md-header__button md-logo" aria-label="Resumos UFMG" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Resumos UFMG
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Aula 12   Aprendizado por Refor√ßo
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Buscar" placeholder="Buscar" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Pesquisar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpar" aria-label="Limpar" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando a pesquisa
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navega√ß√£o" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Resumos UFMG" class="md-nav__button md-logo" aria-label="Resumos UFMG" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Resumos UFMG
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    In√≠cio
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mat√©rias
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Mat√©rias
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filosofia.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="√çndice">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      √çndice
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-definicao-do-problema" class="md-nav__link">
    <span class="md-ellipsis">
      1. Defini√ß√£o do Problema
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-quando-usar-o-aprendizado-por-reforco" class="md-nav__link">
    <span class="md-ellipsis">
      2. Quando Usar o Aprendizado por Refor√ßo?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-principais-abordagens" class="md-nav__link">
    <span class="md-ellipsis">
      3. Principais Abordagens
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-aprendizado-temporal-difference-td-learning" class="md-nav__link">
    <span class="md-ellipsis">
      4. Aprendizado Temporal Difference (TD Learning)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-funcao-de-valor-e-q-function" class="md-nav__link">
    <span class="md-ellipsis">
      5. Fun√ß√£o de Valor e Q-Function
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      6. Q-Learning
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-exploracao-vs-exploracao-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      7. Explora√ß√£o vs Explora√ß√£o (Exploration vs Exploitation)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-exemplos-de-aplicacao" class="md-nav__link">
    <span class="md-ellipsis">
      8. Exemplos de Aplica√ß√£o
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resumo-geral" class="md-nav__link">
    <span class="md-ellipsis">
      Resumo Geral
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p><strong>Aprendizado por Refor√ßo (Reinforcement Learning - RL)</strong> √© um paradigma onde <strong>agentes aprendem ao interagir com o ambiente</strong>, tomando <strong>a√ß√µes</strong> e recebendo <strong>recompensas</strong> de acordo com os resultados.  </p>
<p>üìå <strong>Objetivo</strong>: Aprender a pol√≠tica <strong>√≥tima</strong> de a√ß√µes que <strong>maximiza a recompensa total</strong> ao longo do tempo.</p>
<hr />
<h2 id="1-definicao-do-problema">1. <strong>Defini√ß√£o do Problema</strong></h2>
<p>O Aprendizado por Refor√ßo pode ser modelado como um <strong>Processo de Decis√£o de Markov (MDP)</strong>.</p>
<p>üìå <strong>Elementos do RL</strong>:
‚úÖ <strong>Estados (S)</strong> ‚Üí Representa√ß√£o do ambiente em um dado momento.<br />
‚úÖ <strong>A√ß√µes (A)</strong> ‚Üí Conjunto de escolhas poss√≠veis para o agente.<br />
‚úÖ <strong>Recompensas (R)</strong> ‚Üí Feedback do ambiente ap√≥s uma a√ß√£o.<br />
‚úÖ <strong>Pol√≠tica (œÄ)</strong> ‚Üí Regra que define a <strong>a√ß√£o tomada</strong> para cada estado.<br />
‚úÖ <strong>Modelo de Transi√ß√£o (T)</strong> ‚Üí Probabilidade de transi√ß√£o entre estados (pode ser desconhecido).  </p>
<p>üí° <strong>Diferen√ßa do Aprendizado Supervisionado</strong>:<br />
- No <strong>supervisionado</strong>, h√° um conjunto de <strong>dados rotulados</strong>.<br />
- No <strong>refor√ßo</strong>, <strong>o agente descobre a resposta ao interagir com o ambiente</strong>.  </p>
<hr />
<h2 id="2-quando-usar-o-aprendizado-por-reforco">2. <strong>Quando Usar o Aprendizado por Refor√ßo?</strong></h2>
<p>üìå O RL √© indicado quando:
‚úÖ <strong>Os dados possuem depend√™ncia temporal</strong> (sequ√™ncias ou trajet√≥rias).<br />
‚úÖ <strong>As decis√µes do agente influenciam estados futuros</strong>.<br />
‚úÖ <strong>O modelo do ambiente n√£o √© totalmente conhecido</strong>.  </p>
<p>üí° <strong>Exemplo</strong>: Treinar um <strong>rob√¥ para andar</strong> ‚Üí O rob√¥ aprende quais movimentos s√£o mais eficientes para evitar quedas e avan√ßar.</p>
<hr />
<h2 id="3-principais-abordagens">3. <strong>Principais Abordagens</strong></h2>
<p>O RL pode ser resolvido por diferentes abordagens:</p>
<p>‚úÖ <strong>Programa√ß√£o Din√¢mica</strong> ‚Üí Se o modelo do ambiente for conhecido, pode-se calcular a pol√≠tica √≥tima com:
   - <strong>Value Iteration</strong>
   - <strong>Policy Iteration</strong></p>
<p>‚úÖ <strong>M√©todos Baseados em Amostragem</strong> (quando o modelo n√£o √© conhecido):
   - <strong>Monte Carlo</strong> ‚Üí Aprendizado a partir de epis√≥dios completos.
   - <strong>Temporal Difference (TD Learning)</strong> ‚Üí Atualiza estimativas de recompensa ao longo do tempo.
     - <strong>Q-Learning</strong> (off-policy)
     - <strong>SARSA</strong> (on-policy)</p>
<p>‚úÖ <strong>Aproxima√ß√£o de Fun√ß√µes</strong>:
   - <strong>Modelos Lineares</strong>
   - <strong>Deep Reinforcement Learning (Deep RL)</strong> (usando redes neurais).</p>
<hr />
<h2 id="4-aprendizado-temporal-difference-td-learning">4. <strong>Aprendizado Temporal Difference (TD Learning)</strong></h2>
<p>üìå <strong>TD Learning</strong> combina elementos de:
‚úÖ <strong>Monte Carlo</strong> ‚Üí Atualiza valores com base na experi√™ncia.<br />
‚úÖ <strong>Programa√ß√£o Din√¢mica</strong> ‚Üí Utiliza estimativas para aprender valores de estado.</p>
<p>A atualiza√ß√£o √© feita pela <strong>diferen√ßa temporal</strong> entre a previs√£o e a recompensa observada.</p>
<p>üìå <strong>F√≥rmula do TD Update</strong>:</p>
<p>$$
V(s) \leftarrow V(s) + \alpha \left[ R + \gamma V(s') - V(s) \right]
$$</p>
<p>Onde:
- <strong>$V(s)$</strong> ‚Üí Valor estimado do estado atual.
- <strong>$\alpha$</strong> ‚Üí Taxa de aprendizado.
- <strong>$R$</strong> ‚Üí Recompensa imediata.
- <strong>$\gamma$</strong> ‚Üí Fator de desconto (valoriza√ß√£o de recompensas futuras).
- <strong>$V(s')$</strong> ‚Üí Valor estimado do pr√≥ximo estado.</p>
<p>üí° <strong>O erro de TD mede a diferen√ßa entre o valor atual e a expectativa futura</strong>.</p>
<hr />
<h2 id="5-funcao-de-valor-e-q-function">5. <strong>Fun√ß√£o de Valor e Q-Function</strong></h2>
<p>üìå <strong>Fun√ß√£o de Valor de Estado $V(s)$</strong>: Mede <strong>o valor esperado</strong> das recompensas ao seguir uma pol√≠tica <strong>œÄ</strong> a partir do estado <strong>s</strong>.</p>
<p>$$
V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid s_0 = s \right]
$$</p>
<p>üìå <strong>Fun√ß√£o de Valor de A√ß√µes $Q(s,a)$</strong>: Mede <strong>o valor esperado</strong> ao escolher uma a√ß√£o <strong>a</strong> em um estado <strong>s</strong>, seguindo uma pol√≠tica <strong>œÄ</strong>.</p>
<p>$$
Q^\pi(s,a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t \mid s_0 = s, a_0 = a \right]
$$</p>
<p>üí° <strong>Diferen√ßa</strong>:
‚úÖ <strong>$V(s)$</strong> ‚Üí Mede o <strong>valor do estado</strong>.<br />
‚úÖ <strong>$Q(s,a)$</strong> ‚Üí Mede o <strong>valor de uma a√ß√£o</strong> em um estado.</p>
<hr />
<h2 id="6-q-learning">6. <strong>Q-Learning</strong></h2>
<p>O <strong>Q-Learning</strong> √© um dos algoritmos mais usados em RL. Ele permite aprender <strong>Q(s, a)</strong> <strong>sem precisar conhecer o modelo de transi√ß√£o</strong>.</p>
<p>üìå <strong>F√≥rmula de Atualiza√ß√£o do Q-Learning</strong>:</p>
<p>$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$</p>
<p>üí° <strong>Caracter√≠sticas</strong>:
‚úÖ <strong>Off-policy</strong> ‚Üí Aprende com a <strong>melhor a√ß√£o futura poss√≠vel</strong> e n√£o apenas com as a√ß√µes tomadas pelo agente.<br />
‚úÖ <strong>Model-free</strong> ‚Üí N√£o precisa conhecer a fun√ß√£o de transi√ß√£o do ambiente.  </p>
<p>üìå <strong>Passos do Algoritmo Q-Learning</strong>:
1. Inicializa <strong>$Q(s,a)$</strong> para todos os estados e a√ß√µes.
2. No estado <strong>s</strong>, escolhe uma a√ß√£o <strong>a</strong>.
3. Executa <strong>a</strong>, recebe <strong>recompensa R</strong> e observa <strong>novo estado s'</strong>.
4. Atualiza <strong>$Q(s,a)$</strong> usando a equa√ß√£o acima.
5. Repete at√© converg√™ncia.</p>
<hr />
<h2 id="7-exploracao-vs-exploracao-exploration-vs-exploitation">7. <strong>Explora√ß√£o vs Explora√ß√£o (Exploration vs Exploitation)</strong></h2>
<p>Para aprender bem, o agente precisa equilibrar:
‚úÖ <strong>Explora√ß√£o (Exploration)</strong> ‚Üí Testar novas a√ß√µes para descobrir melhores estrat√©gias.<br />
‚úÖ <strong>Explora√ß√£o (Exploitation)</strong> ‚Üí Escolher as melhores a√ß√µes j√° conhecidas.</p>
<p>üìå <strong>T√©cnica $\epsilon$-Greedy</strong>:
6. Com probabilidade <strong>$\epsilon$</strong>, escolhe uma a√ß√£o aleat√≥ria (explora√ß√£o).
7. Com probabilidade <strong>$1 - \epsilon$</strong>, escolhe <strong>a melhor a√ß√£o conhecida</strong> (explora√ß√£o).</p>
<p>üí° <strong>Inicialmente $\epsilon$ deve ser alto para mais explora√ß√£o, e depois diminuir para mais explora√ß√£o.</strong></p>
<hr />
<h2 id="8-exemplos-de-aplicacao">8. <strong>Exemplos de Aplica√ß√£o</strong></h2>
<p>‚úÖ <strong>Grid World</strong> ‚Üí O agente aprende a navegar em um ambiente para alcan√ßar <strong>a maior recompensa</strong>.<br />
‚úÖ <strong>Pac-Man</strong> ‚Üí O agente aprende estrat√©gias para evitar fantasmas e maximizar a pontua√ß√£o.<br />
‚úÖ <strong>Rob√≥tica</strong> ‚Üí Treinamento de rob√¥s para locomo√ß√£o eficiente.<br />
‚úÖ <strong>Financeiro</strong> ‚Üí Algoritmos que aprendem estrat√©gias de negocia√ß√£o.<br />
‚úÖ <strong>Jogos</strong> ‚Üí Aprendizado de IA em <strong>Xadrez, Go, Starcraft e Dota 2</strong>.</p>
<hr />
<h1 id="resumo-geral"><strong>Resumo Geral</strong></h1>
<p>üìå <strong>Aprendizado por Refor√ßo (RL)</strong>:
- Baseado em <strong>intera√ß√£o com o ambiente</strong> e <strong>recebimento de recompensas</strong>.
- Pode ser modelado como um <strong>MDP</strong>.
- Objetivo: <strong>Encontrar a melhor pol√≠tica $\pi^*$ para maximizar as recompensas</strong>.</p>
<p>üìå <strong>Principais T√©cnicas</strong>:
‚úÖ <strong>Temporal Difference Learning</strong> ‚Üí Atualiza valores de estado com base na diferen√ßa entre previs√µes e recompensas reais.<br />
‚úÖ <strong>Q-Learning</strong> ‚Üí Algoritmo <strong>off-policy</strong>, baseado na atualiza√ß√£o da fun√ß√£o de valor de a√ß√£o.<br />
‚úÖ <strong>Explora√ß√£o vs Explora√ß√£o</strong> ‚Üí Estrat√©gias como <strong>$\epsilon$-Greedy</strong> balanceiam descoberta e aproveitamento.</p>
<p>üöÄ <strong>Aplica√ß√µes pr√°ticas incluem</strong>: Jogos, rob√≥tica, finan√ßas, assistentes inteligentes e mu</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copiado para \u00e1rea de transfer\u00eancia", "clipboard.copy": "Copiar para \u00e1rea de transfer\u00eancia", "search.result.more.one": "Mais 1 nesta p\u00e1gina", "search.result.more.other": "Mais # nesta p\u00e1gina", "search.result.none": "Nenhum resultado encontrado", "search.result.one": "1 resultado encontrado", "search.result.other": "# resultados encontrados", "search.result.placeholder": "Digite para iniciar a busca", "search.result.term.missing": "Ausente", "select.version": "Selecione a vers\u00e3o"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>