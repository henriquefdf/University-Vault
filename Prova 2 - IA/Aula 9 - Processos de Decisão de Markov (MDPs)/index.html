
<!doctype html>
<html lang="pt" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.1">
    
    
      
        <title>Aula 9 Processos de Decis√£o de Markov (MDPs) - Resumos UFMG</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.a40c8224.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-tomada-de-decisao" class="md-skip">
          Ir para o conte√∫do
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Cabe√ßalho">
    <a href="../.." title="Resumos UFMG" class="md-header__button md-logo" aria-label="Resumos UFMG" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Resumos UFMG
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Aula 9   Processos de Decis√£o de Markov (MDPs)
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Buscar" placeholder="Buscar" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Pesquisar">
        
        <button type="reset" class="md-search__icon md-icon" title="Limpar" aria-label="Limpar" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Inicializando a pesquisa
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navega√ß√£o" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Resumos UFMG" class="md-nav__button md-logo" aria-label="Resumos UFMG" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Resumos UFMG
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    In√≠cio
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Mat√©rias
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Mat√©rias
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../filosofia.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    IA
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="√çndice">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      √çndice
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-tomada-de-decisao" class="md-nav__link">
    <span class="md-ellipsis">
      1. Tomada de Decis√£o
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Tomada de Decis√£o">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problema" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Problema
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-multi-armed-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      2. Multi-Armed Bandits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Multi-Armed Bandits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#caracteristicas" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Caracter√≠sticas
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estrategias" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Estrat√©gias
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-processos-de-decisao-de-markov-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      3. Processos de Decis√£o de Markov (MDPs)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Processos de Decis√£o de Markov (MDPs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#propriedade-de-markov" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Propriedade de Markov
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#definicao-formal" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Defini√ß√£o Formal
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exemplo-mundo-4x3" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Exemplo: Mundo 4x3
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-recompensas-acumuladas-e-descontadas" class="md-nav__link">
    <span class="md-ellipsis">
      4. Recompensas Acumuladas e Descontadas
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Recompensas Acumuladas e Descontadas">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#recompensa-cumulativa" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Recompensa Cumulativa
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recompensa-descontada" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Recompensa Descontada
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-politicas-de-decisao" class="md-nav__link">
    <span class="md-ellipsis">
      5. Pol√≠ticas de Decis√£o
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Pol√≠ticas de Decis√£o">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exemplo-de-politica" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Exemplo de Pol√≠tica
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-funcao-de-valor" class="md-nav__link">
    <span class="md-ellipsis">
      6. Fun√ß√£o de Valor
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Fun√ß√£o de Valor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#equacao-de-bellman" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ Equa√ß√£o de Bellman
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-politicas-e-funcoes-de-valor-otimas" class="md-nav__link">
    <span class="md-ellipsis">
      7. Pol√≠ticas e Fun√ß√µes de Valor √ìtimas
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-programacao-dinamica-para-resolver-mdps" class="md-nav__link">
    <span class="md-ellipsis">
      8. Programa√ß√£o Din√¢mica para Resolver MDPs
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Programa√ß√£o Din√¢mica para Resolver MDPs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#metodos-de-programacao-dinamica" class="md-nav__link">
    <span class="md-ellipsis">
      üîπ M√©todos de Programa√ß√£o Din√¢mica
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-iteracao-de-valor" class="md-nav__link">
    <span class="md-ellipsis">
      9. Itera√ß√£o de Valor
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusao" class="md-nav__link">
    <span class="md-ellipsis">
      Conclus√£o
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p>Os <strong>Processos de Decis√£o de Markov (MDPs)</strong> s√£o usados para modelar agentes que tomam <strong>decis√µes sequenciais</strong> em ambientes incertos, onde cada a√ß√£o influencia estados futuros. Eles s√£o amplamente utilizados em <strong>Aprendizado por Refor√ßo (Reinforcement Learning)</strong>.</p>
<hr />
<h2 id="1-tomada-de-decisao">1. Tomada de Decis√£o</h2>
<h3 id="problema">üîπ Problema</h3>
<ul>
<li>Um agente est√° em um <strong>estado</strong> e pode escolher entre v√°rias <strong>a√ß√µes</strong>.</li>
<li>Seu objetivo √© escolher a <strong>a√ß√£o que maximiza a recompensa acumulada</strong>.</li>
</ul>
<p>Existem dois tipos principais de tomada de decis√£o:
1. <strong>Decis√£o √∫nica ("single-shot")</strong>: Problema dos <strong>Multi-Armed Bandits</strong>.
2. <strong>Decis√£o sequencial</strong>: Modelada por <strong>Processos de Decis√£o de Markov (MDPs)</strong>.</p>
<hr />
<h2 id="2-multi-armed-bandits">2. Multi-Armed Bandits</h2>
<p>Um <strong>Multi-Armed Bandit</strong> √© uma met√°fora para um agente que precisa decidir entre v√°rias op√ß√µes desconhecidas.</p>
<h3 id="caracteristicas">üîπ Caracter√≠sticas</h3>
<ul>
<li>Cada <strong>alavanca</strong> de uma m√°quina ca√ßa-n√≠quel d√° um <strong>recompensa aleat√≥ria</strong> com <strong>m√©dia e vari√¢ncia diferentes</strong>.</li>
<li>O agente deve decidir <strong>quando explorar novas op√ß√µes</strong> e <strong>quando explorar a melhor op√ß√£o conhecida</strong> (<strong>dilema explora√ß√£o x explora√ß√£o</strong>).</li>
</ul>
<h3 id="estrategias">üîπ Estrat√©gias</h3>
<ol>
<li><strong>Œµ-greedy</strong>:  </li>
<li>Com probabilidade <strong>Œµ</strong>, escolhe uma a√ß√£o aleat√≥ria (explora√ß√£o).  </li>
<li>
<p>Com probabilidade <strong>1 - Œµ</strong>, escolhe a a√ß√£o de maior recompensa (explora√ß√£o).</p>
</li>
<li>
<p><strong>Upper Confidence Bound (UCB)</strong>:  </p>
</li>
<li>Escolhe a a√ß√£o com maior <strong>m√©dia + vari√¢ncia</strong>, equilibrando explora√ß√£o e explora√ß√£o.</li>
</ol>
<hr />
<h2 id="3-processos-de-decisao-de-markov-mdps">3. Processos de Decis√£o de Markov (MDPs)</h2>
<p>Os <strong>MDPs</strong> s√£o modelos matem√°ticos usados para representar decis√µes sequenciais sob incerteza.</p>
<h3 id="propriedade-de-markov">üîπ <strong>Propriedade de Markov</strong></h3>
<p>A <strong>transi√ß√£o de estado</strong> depende apenas do <strong>estado atual</strong> e da <strong>a√ß√£o tomada</strong>, <strong>n√£o dos estados passados</strong>.</p>
<p>$$
P(s' | s, a) = T(s, a, s')
$$</p>
<h3 id="definicao-formal">üîπ <strong>Defini√ß√£o Formal</strong></h3>
<p>Um <strong>MDP</strong> √© definido como um <strong>tupla</strong> ‚ü®S, A, T, R‚ü©:
- <strong>S</strong>: Conjunto de <strong>estados</strong>.
- <strong>A</strong>: Conjunto de <strong>a√ß√µes</strong>.
- <strong>T</strong>: <strong>Fun√ß√£o de transi√ß√£o</strong> ‚Üí Probabilidade de chegar ao estado <strong>s'</strong> ao tomar a a√ß√£o <strong>a</strong> no estado <strong>s</strong>:</p>
<p>$$
  T(s,a,s') = P(s' | s,a)
  $$</p>
<ul>
<li><strong>R</strong>: <strong>Fun√ß√£o de recompensa</strong> ‚Üí Recompensa recebida ao estar no estado <strong>s</strong>:</li>
</ul>
<p>$$
  R(s)
  $$</p>
<h3 id="exemplo-mundo-4x3">üîπ <strong>Exemplo: Mundo 4x3</strong></h3>
<ul>
<li><strong>Estados (S)</strong>: { (1,1), (1,2), ..., (4,3) }.</li>
<li><strong>A√ß√µes (A)</strong>: { Cima, Baixo, Esquerda, Direita }.</li>
<li><strong>Recompensas (R)</strong>:</li>
<li>+1 no estado (4,3).</li>
<li>-1 no estado (4,2).</li>
<li>-0.04 nos demais estados.</li>
</ul>
<hr />
<h2 id="4-recompensas-acumuladas-e-descontadas">4. Recompensas Acumuladas e Descontadas</h2>
<p>Ao longo do tempo, um agente <strong>acumula recompensas</strong> √† medida que interage com o ambiente.</p>
<h3 id="recompensa-cumulativa">üîπ <strong>Recompensa Cumulativa</strong></h3>
<p>Para um horizonte <strong>finito</strong> (T passos), a recompensa acumulada √©:</p>
<p>$$
G_t = R_{t+1} + R_{t+2} + ... + R_T
$$</p>
<h3 id="recompensa-descontada">üîπ <strong>Recompensa Descontada</strong></h3>
<p>Para um horizonte <strong>infinito</strong>, recompensas futuras s√£o <strong>penalizadas</strong> por um <strong>fator de desconto Œ≥</strong>:</p>
<p>$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...
$$</p>
<ul>
<li><strong>Œ≥ (fator de desconto)</strong>:  </li>
<li><strong>Œ≥ ‚Üí 0</strong> ‚Üí O agente <strong>s√≥ valoriza recompensas imediatas</strong>.  </li>
<li><strong>Œ≥ ‚Üí 1</strong> ‚Üí O agente <strong>valoriza tamb√©m recompensas futuras</strong>.</li>
</ul>
<hr />
<h2 id="5-politicas-de-decisao">5. Pol√≠ticas de Decis√£o</h2>
<p>Uma <strong>pol√≠tica</strong> œÄ define o comportamento do agente.</p>
<ul>
<li><strong>Defini√ß√£o</strong>: Mapeia estados para a√ß√µes.
  $$
  a = \pi(s)
  $$</li>
<li><strong>Objetivo</strong>: Encontrar a <strong>pol√≠tica √≥tima</strong> œÄ* que maximiza a recompensa ao longo do tempo.</li>
</ul>
<h3 id="exemplo-de-politica">üîπ <strong>Exemplo de Pol√≠tica</strong></h3>
<ul>
<li>Se o agente segue a pol√≠tica:</li>
<li><strong>œÄ(s) = direita</strong> no estado (1,1).</li>
<li><strong>œÄ(s) = cima</strong> no estado (2,1).</li>
<li>...</li>
</ul>
<p>Isso define uma estrat√©gia para navegar no ambiente.</p>
<hr />
<h2 id="6-funcao-de-valor">6. Fun√ß√£o de Valor</h2>
<p>A <strong>fun√ß√£o de valor</strong> mede a <strong>qualidade de um estado</strong> a longo prazo.</p>
<ul>
<li><strong>Defini√ß√£o</strong>: O valor de um estado <strong>s</strong> √© a soma esperada das recompensas descontadas ao seguir a pol√≠tica œÄ:</li>
</ul>
<p>$$
  V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t) \mid s_0 = s \right]
  $$</p>
<h3 id="equacao-de-bellman">üîπ <strong>Equa√ß√£o de Bellman</strong></h3>
<p>A fun√ß√£o de valor pode ser definida <strong>recursivamente</strong>:</p>
<p>$$
V^\pi(s) = R(s) + \gamma \sum_{s'} P(s' | s, \pi(s)) V^\pi(s')
$$</p>
<p>Isso permite calcular valores de estados <strong>de maneira iterativa</strong>.</p>
<hr />
<h2 id="7-politicas-e-funcoes-de-valor-otimas">7. Pol√≠ticas e Fun√ß√µes de Valor √ìtimas</h2>
<p>A <strong>pol√≠tica √≥tima</strong> √© aquela que <strong>maximiza a fun√ß√£o de valor</strong>:</p>
<p>$$
V^<em>(s) = \max_a \sum_{s'} P(s' | s, a) [ R(s) + \gamma V^</em>(s') ]
$$</p>
<ul>
<li>A pol√≠tica √≥tima <strong>œÄ</strong><em> √© obtida escolhendo </em><em>sempre a melhor a√ß√£o</em>*.</li>
<li>Essa equa√ß√£o √© conhecida como a <strong>Equa√ß√£o da Otimalidade de Bellman</strong>.</li>
</ul>
<hr />
<h2 id="8-programacao-dinamica-para-resolver-mdps">8. Programa√ß√£o Din√¢mica para Resolver MDPs</h2>
<p>Se conhecemos a fun√ß√£o de transi√ß√£o <strong>T(s, a, s')</strong> e a fun√ß√£o de recompensa <strong>R(s)</strong>, podemos encontrar a pol√≠tica √≥tima com <strong>Programa√ß√£o Din√¢mica</strong>.</p>
<h3 id="metodos-de-programacao-dinamica">üîπ M√©todos de Programa√ß√£o Din√¢mica</h3>
<ol>
<li><strong>Itera√ß√£o de Valor (Value Iteration)</strong>  </li>
<li>Inicia com valores aleat√≥rios e atualiza iterativamente usando a <strong>Equa√ß√£o de Bellman</strong>.</li>
<li>
<p>Converge para <strong>V</strong><em> e </em><em>œÄ</em>**.</p>
</li>
<li>
<p><strong>Itera√ß√£o de Pol√≠tica (Policy Iteration)</strong>  </p>
</li>
<li>Alterna entre <strong>avalia√ß√£o da pol√≠tica</strong> e <strong>melhoria da pol√≠tica</strong>.</li>
<li>Garante converg√™ncia para a pol√≠tica √≥tima.</li>
</ol>
<hr />
<h2 id="9-iteracao-de-valor">9. Itera√ß√£o de Valor</h2>
<p>O algoritmo de <strong>Value Iteration</strong> funciona da seguinte forma:</p>
<ol>
<li>Inicializa <strong>V(s)</strong> aleatoriamente para todos os estados.</li>
<li>Para cada estado <strong>s</strong>, aplica a <strong>Equa√ß√£o da Otimalidade de Bellman</strong>:</li>
</ol>
<p>$$
   V(s) \leftarrow \max_a \sum_{s'} P(s' | s, a) [ R(s) + \gamma V(s') ]
   $$</p>
<ol>
<li>Repete at√© que <strong>as mudan√ßas sejam pequenas</strong> (abaixo de um limite).</li>
</ol>
<hr />
<h1 id="conclusao">Conclus√£o</h1>
<p>Os <strong>Processos de Decis√£o de Markov (MDPs)</strong> s√£o fundamentais para a modelagem de <strong>decis√µes sequenciais</strong>. Eles fornecem um arcabou√ßo matem√°tico para <strong>agentes que aprendem a agir em ambientes incertos</strong>.</p>
<p>‚úÖ <strong>Modelam problemas de decis√£o sob incerteza</strong><br />
‚úÖ <strong>Usam recompensas descontadas para avaliar estrat√©gias</strong><br />
‚úÖ <strong>Podem ser resolvidos via Programa√ß√£o Din√¢mica</strong>  </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copiado para \u00e1rea de transfer\u00eancia", "clipboard.copy": "Copiar para \u00e1rea de transfer\u00eancia", "search.result.more.one": "Mais 1 nesta p\u00e1gina", "search.result.more.other": "Mais # nesta p\u00e1gina", "search.result.none": "Nenhum resultado encontrado", "search.result.one": "1 resultado encontrado", "search.result.other": "# resultados encontrados", "search.result.placeholder": "Digite para iniciar a busca", "search.result.term.missing": "Ausente", "select.version": "Selecione a vers\u00e3o"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.5090c770.min.js"></script>
      
    
  </body>
</html>