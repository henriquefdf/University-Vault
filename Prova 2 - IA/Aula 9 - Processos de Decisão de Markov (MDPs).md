Os **Processos de Decis√£o de Markov (MDPs)** s√£o usados para modelar agentes que tomam **decis√µes sequenciais** em ambientes incertos, onde cada a√ß√£o influencia estados futuros. Eles s√£o amplamente utilizados em **Aprendizado por Refor√ßo (Reinforcement Learning)**.

---

## 1. Tomada de Decis√£o

### üîπ Problema
- Um agente est√° em um **estado** e pode escolher entre v√°rias **a√ß√µes**.
- Seu objetivo √© escolher a **a√ß√£o que maximiza a recompensa acumulada**.

Existem dois tipos principais de tomada de decis√£o:
1. **Decis√£o √∫nica ("single-shot")**: Problema dos **Multi-Armed Bandits**.
2. **Decis√£o sequencial**: Modelada por **Processos de Decis√£o de Markov (MDPs)**.

---

## 2. Multi-Armed Bandits

Um **Multi-Armed Bandit** √© uma met√°fora para um agente que precisa decidir entre v√°rias op√ß√µes desconhecidas.

### üîπ Caracter√≠sticas
- Cada **alavanca** de uma m√°quina ca√ßa-n√≠quel d√° um **recompensa aleat√≥ria** com **m√©dia e vari√¢ncia diferentes**.
- O agente deve decidir **quando explorar novas op√ß√µes** e **quando explorar a melhor op√ß√£o conhecida** (**dilema explora√ß√£o x explora√ß√£o**).

### üîπ Estrat√©gias
1. **Œµ-greedy**:  
   - Com probabilidade **Œµ**, escolhe uma a√ß√£o aleat√≥ria (explora√ß√£o).  
   - Com probabilidade **1 - Œµ**, escolhe a a√ß√£o de maior recompensa (explora√ß√£o).

2. **Upper Confidence Bound (UCB)**:  
   - Escolhe a a√ß√£o com maior **m√©dia + vari√¢ncia**, equilibrando explora√ß√£o e explora√ß√£o.

---

## 3. Processos de Decis√£o de Markov (MDPs)

Os **MDPs** s√£o modelos matem√°ticos usados para representar decis√µes sequenciais sob incerteza.

### üîπ **Propriedade de Markov**
A **transi√ß√£o de estado** depende apenas do **estado atual** e da **a√ß√£o tomada**, **n√£o dos estados passados**.

$$
P(s' | s, a) = T(s, a, s')
$$

### üîπ **Defini√ß√£o Formal**
Um **MDP** √© definido como um **tupla** ‚ü®S, A, T, R‚ü©:
- **S**: Conjunto de **estados**.
- **A**: Conjunto de **a√ß√µes**.
- **T**: **Fun√ß√£o de transi√ß√£o** ‚Üí Probabilidade de chegar ao estado **s'** ao tomar a a√ß√£o **a** no estado **s**:
  
  $$
  T(s,a,s') = P(s' | s,a)
  $$

- **R**: **Fun√ß√£o de recompensa** ‚Üí Recompensa recebida ao estar no estado **s**:

  $$
  R(s)
  $$

### üîπ **Exemplo: Mundo 4x3**
- **Estados (S)**: { (1,1), (1,2), ..., (4,3) }.
- **A√ß√µes (A)**: { Cima, Baixo, Esquerda, Direita }.
- **Recompensas (R)**:
  - +1 no estado (4,3).
  - -1 no estado (4,2).
  - -0.04 nos demais estados.

---

## 4. Recompensas Acumuladas e Descontadas

Ao longo do tempo, um agente **acumula recompensas** √† medida que interage com o ambiente.

### üîπ **Recompensa Cumulativa**
Para um horizonte **finito** (T passos), a recompensa acumulada √©:

$$
G_t = R_{t+1} + R_{t+2} + ... + R_T
$$

### üîπ **Recompensa Descontada**
Para um horizonte **infinito**, recompensas futuras s√£o **penalizadas** por um **fator de desconto Œ≥**:

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...
$$

- **Œ≥ (fator de desconto)**:  
  - **Œ≥ ‚Üí 0** ‚Üí O agente **s√≥ valoriza recompensas imediatas**.  
  - **Œ≥ ‚Üí 1** ‚Üí O agente **valoriza tamb√©m recompensas futuras**.

---

## 5. Pol√≠ticas de Decis√£o

Uma **pol√≠tica** œÄ define o comportamento do agente.

- **Defini√ß√£o**: Mapeia estados para a√ß√µes.
  $$
  a = \pi(s)
  $$
- **Objetivo**: Encontrar a **pol√≠tica √≥tima** œÄ* que maximiza a recompensa ao longo do tempo.

### üîπ **Exemplo de Pol√≠tica**
- Se o agente segue a pol√≠tica:
  - **œÄ(s) = direita** no estado (1,1).
  - **œÄ(s) = cima** no estado (2,1).
  - ...
  
  Isso define uma estrat√©gia para navegar no ambiente.

---

## 6. Fun√ß√£o de Valor

A **fun√ß√£o de valor** mede a **qualidade de um estado** a longo prazo.

- **Defini√ß√£o**: O valor de um estado **s** √© a soma esperada das recompensas descontadas ao seguir a pol√≠tica œÄ:

  $$
  V^\pi(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R(s_t) \mid s_0 = s \right]
  $$

### üîπ **Equa√ß√£o de Bellman**
A fun√ß√£o de valor pode ser definida **recursivamente**:

$$
V^\pi(s) = R(s) + \gamma \sum_{s'} P(s' | s, \pi(s)) V^\pi(s')
$$

Isso permite calcular valores de estados **de maneira iterativa**.

---

## 7. Pol√≠ticas e Fun√ß√µes de Valor √ìtimas

A **pol√≠tica √≥tima** √© aquela que **maximiza a fun√ß√£o de valor**:

$$
V^*(s) = \max_a \sum_{s'} P(s' | s, a) [ R(s) + \gamma V^*(s') ]
$$

- A pol√≠tica √≥tima **œÄ*** √© obtida escolhendo **sempre a melhor a√ß√£o**.
- Essa equa√ß√£o √© conhecida como a **Equa√ß√£o da Otimalidade de Bellman**.

---

## 8. Programa√ß√£o Din√¢mica para Resolver MDPs

Se conhecemos a fun√ß√£o de transi√ß√£o **T(s, a, s')** e a fun√ß√£o de recompensa **R(s)**, podemos encontrar a pol√≠tica √≥tima com **Programa√ß√£o Din√¢mica**.

### üîπ M√©todos de Programa√ß√£o Din√¢mica
1. **Itera√ß√£o de Valor (Value Iteration)**  
   - Inicia com valores aleat√≥rios e atualiza iterativamente usando a **Equa√ß√£o de Bellman**.
   - Converge para **V*** e **œÄ***.

2. **Itera√ß√£o de Pol√≠tica (Policy Iteration)**  
   - Alterna entre **avalia√ß√£o da pol√≠tica** e **melhoria da pol√≠tica**.
   - Garante converg√™ncia para a pol√≠tica √≥tima.

---

## 9. Itera√ß√£o de Valor

O algoritmo de **Value Iteration** funciona da seguinte forma:

1. Inicializa **V(s)** aleatoriamente para todos os estados.
2. Para cada estado **s**, aplica a **Equa√ß√£o da Otimalidade de Bellman**:
   
   $$
   V(s) \leftarrow \max_a \sum_{s'} P(s' | s, a) [ R(s) + \gamma V(s') ]
   $$

3. Repete at√© que **as mudan√ßas sejam pequenas** (abaixo de um limite).

---

# Conclus√£o

Os **Processos de Decis√£o de Markov (MDPs)** s√£o fundamentais para a modelagem de **decis√µes sequenciais**. Eles fornecem um arcabou√ßo matem√°tico para **agentes que aprendem a agir em ambientes incertos**.

‚úÖ **Modelam problemas de decis√£o sob incerteza**  
‚úÖ **Usam recompensas descontadas para avaliar estrat√©gias**  
‚úÖ **Podem ser resolvidos via Programa√ß√£o Din√¢mica**  